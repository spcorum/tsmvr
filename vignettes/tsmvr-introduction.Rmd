---
title: "An Introduction to Truly Sparse Multivariate Regression"
author: "Sean Corum"
date: "`r Sys.Date()`"
output: prettydoc::html_pretty
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Truly Sparse Multivariate Regression

__tsmvr__ or Truly Sparse Multivariate Regression is an R package for solving sparse multivariate regression problems with error coviance estimation. The workhourse algorithm in __tsmvr__ is adapted from the algorithm described by Chen and Gu in their paper "High Dimensional Multivariate Regression and Precision Matrix Estimation via Nonconvex Optimization" (ArXiv ID 1606.00832, 2016).

A multivariate regression problem is a regression problem with multiple responses. Formally,

$ \mathbf{Y} = \mathbf{X} \mathbf{B} + \mathbf{E} $

Here, \mathbf{X} is the design matrix of $n$ observations of $p$ features, \mathbf{Y} is the design matrix of $n$ observations of $q$ responses, \mathbf{B} is the regression matrix, and \mathbf{E} is the error term. Given  \mathbf{X} and \mathbf{Y}, __tsmvr__ solves this probem for \mathbf{B} under the constraint that \mathbf{B} is sparse and the condition that the errors may be correlated. Under the hood, the error correlations are encoded in the precision matrix \mathbf{\Omega}, which has its own sparsity constraint.

## A First Example

For a first example, define some problem parameters.

```{r first-parameters}
n = 1000                           # number of observations
p = 100                            # number of predictors
q = 10                             # number of responses
sparsity = 0.1                     # sparsity of true regression matrix
s1 = round(p * q * sparsity * 1.1) # fitted sparsity will be a little larger than the true sparsity
s2 = 3 * q - 4                     # constrains precision matrix to have the number of entries as a tri-diagonal matrix
```

The package __tsmvrextras__ contains functions for providing synthetic and real problems in a format ready to be input into __tsmvr__. The following code generates a synthetic dataset. Here, the dataset has a true regression matrix of `r sparsity`. See __tsmvrextras__ documentation for details about how the synthetic data are generated.

```{r first-data}
library(tsmvrextras)
set.seed(1729)
data = tsmvrextras::make_data(
  n = n, p = p, q = q, b1 = sqrt(sparsity), b2 = sqrt(sparsity)
)[[1]]
```

The function \code{tsmvr_solve} solves the regression problem using hard-thresholded block-wise alternating gradient descent.

```{r first-solution-gd-gd}
library(tsmvr)
gd_gd_solution = tsmvr_solve(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, 
  Omega_type = 'gd', eta1 = 0.05, eta2 = 0.2,
  skip = 50
)
```

```{r first-solution-ls-min}
library(tsmvr)
gd_gd_solution = tsmvr_solve(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, 
  B_type = 'ls', Omega_type = 'min', 
  eta1 = 0.05, eta2 = 0.2,
  skip = 50
)
```


```{r test}
library(tsmvr)
library(tsmvrextras)

# A = matrix(c(1,0,0,0,1,0,0,0,1),3,3)
# B = matrix(c(1,1,0,1,1,1,0,1,1),3,3)
C = matrix(c(2,1,0.5,1,2,1,0.5,1,2),3,3)

# htHelper(A,1)
# htHelper(A,2)

# htHelper(B,1)
# htHelper(B,2)

htHelper(C,1)
htHelper(C,2)
htHelper(C,3)
htHelper(C,4)
htHelper(C,5)
htHelper(C,6)
htHelper(C,7)
htHelper(C,8)
htHelper(C,9)

ht(C,1,ss=T)
ht(C,2,ss=T)
ht(C,3,ss=T)
ht(C,4,ss=T)
ht(C,5,ss=T)
ht(C,6,ss=T)
ht(C,7,ss=T)
ht(C,8,ss=T)
ht(C,9,ss=T)

data = tsmvrextras::make_data(
  n = 10, p = 3, q = 3, b1 = 1, b2 = 1
)[[1]]

data$B
ht(data$B,1)
ht(data$B,2)
ht(data$B,3)
ht(data$B,4)
ht(data$B,5)
ht(data$B,6)
ht(data$B,7)
ht(data$B,8)
ht(data$B,9)

data$Omega
ht(data$Omega,1)
ht(data$Omega,2)
ht(data$Omega,3)
ht(data$Omega,4)
ht(data$Omega,5)
ht(data$Omega,6)
ht(data$Omega,7)
ht(data$Omega,8)
ht(data$Omega,9)

ht(data$Omega,1,ss=T)
ht(data$Omega,2,ss=T)
ht(data$Omega,3,ss=T)
ht(data$Omega,4,ss=T)
ht(data$Omega,5,ss=T)
ht(data$Omega,6,ss=T)
ht(data$Omega,7,ss=T)
ht(data$Omega,8,ss=T)
ht(data$Omega,9,ss=T)

n = 1000
p = 100
q = 10
sparsity = 0.1
s1 = round(p * q * sparsity * 1.1)
s2 = 3 * q - 4
data = tsmvrextras::make_data(
  n = n, p = p, q = q, b1 = sqrt(sparsity), b2 = sqrt(sparsity)
)[[1]]

test_solution = tsmvr::tsmvr_solve(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, 
  Omega_type = 'gd', eta1 = 0.05, eta2 = 0.2,
  skip = 50
)
```

For some problems like this one, an equally good solution instead may be found using alternating gradient descent-direct minimization.

```{r first-solution-gd-min, eval = F}
gd_min_solution = tsmvr::tsmvr_solve(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, 
  Omega_type = 'min', eta1 = 0.01,
  skip = 5
)
```

## k-fold Cross Validation

k-fold cross validation may be performed using the function \code{tsmvr_cv}

```{r cross-validated, eval = F}
set.seed(1)
validated = tsmvr::tsmvr_cv(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, k = 5,
  Omega_type = 'gd', eta1 = 0.02, eta2 = 0.2
)
```

## Replicated k-fold Cross Validation

Similarly, replicated k-fold cross validation may be performed using the function \code{tsmvr_replicate}

```{r replicated, eval = F}
set.seed(3)
replicated = tsmvr::tsmvr_replicate(
  X = data$X, Y = data$Y, s1 = s1, s2 = s2, k = 5, reps = 3,
  Omega_type = 'gd', eta1 = 0.02, eta2 = 0.2
)
```

## Gridsearch

Finally, replicated k-fold cross validation may be used to search a space of \code{s1} and \code{s2} values to find the pair of values that minimizes the cross validation error.

```{r gridsearch, eval = F}
library(tsmvrextras)
library(tsmvr)
s1_grid = p*q*0.11
s2_grid = 3*q-4

s1_grid = round(p*q*exp(log(seq(0.1,0.2,length.out = 2))))
s2_grid = round(seq(2*q,4*q,length.out = 2))
set.seed(5)

grid = tsmvr_gridsearch(
  X = data$X, Y = data$Y, s1_grid = s1_grid, s2_grid = s2_grid, 
  k = 5, reps = 3, Omega_type = 'gd', eta1 = 0.02, eta2 = 0.2,
  quiet = F
)
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
